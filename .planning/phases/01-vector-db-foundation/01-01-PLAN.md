---
phase: 01-vector-db-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - docker-compose.yml
  - execution/vector_db/__init__.py
  - execution/vector_db/models.py
  - execution/vector_db/init.sql
  - execution/vector_db/connection.py
  - execution/config.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "PostgreSQL with pgvector runs locally via Docker Compose"
    - "SQLAlchemy models define the embedding schema with tenant isolation"
    - "Database connection pool is configurable and tested"
    - "Existing SQLite pipeline (reddit_content.db) is unaffected"
  artifacts:
    - path: "docker-compose.yml"
      provides: "pgvector PostgreSQL container definition"
      contains: "pgvector/pgvector:pg17"
    - path: "execution/vector_db/models.py"
      provides: "SQLAlchemy ORM models for knowledge base"
      exports: ["Base", "KnowledgeChunk", "Document", "IngestionLog"]
      min_lines: 80
    - path: "execution/vector_db/init.sql"
      provides: "Database initialization with pgvector extension"
      contains: "CREATE EXTENSION IF NOT EXISTS vector"
    - path: "execution/vector_db/connection.py"
      provides: "Async-capable database session factory"
      exports: ["get_engine", "get_session", "init_db"]
    - path: "execution/config.py"
      provides: "VectorDBConfig section added to GhostWriterConfig"
      contains: "class VectorDBConfig"
  key_links:
    - from: "execution/vector_db/models.py"
      to: "execution/vector_db/connection.py"
      via: "shared Base declarative base"
      pattern: "from.*models.*import.*Base"
    - from: "execution/vector_db/connection.py"
      to: "execution/config.py"
      via: "reads database URL from config"
      pattern: "config\\.vector_db"
    - from: "docker-compose.yml"
      to: "execution/vector_db/init.sql"
      via: "docker-entrypoint-initdb.d mount"
      pattern: "docker-entrypoint-initdb"
---

<objective>
Set up PostgreSQL with pgvector via Docker Compose and create the SQLAlchemy data model for the knowledge base.

Purpose: This is the foundation for the entire knowledge layer. Every subsequent plan depends on having a running vector database with a well-defined schema. The data model must support tenant isolation, relationship tracking, and efficient vector similarity search from day one.

Output: Working Docker PostgreSQL container with pgvector extension, SQLAlchemy models for knowledge chunks and documents, database connection management, and config extensions.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-vector-db-foundation/01-RESEARCH.md
@.planning/phases/01-vector-db-foundation/01-CONTEXT.md
@execution/config.py
@execution/sources/database.py
@execution/sources/base_source.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Docker Compose + pgvector initialization</name>
  <files>
    docker-compose.yml
    execution/vector_db/init.sql
  </files>
  <action>
    Create `docker-compose.yml` at project root with:
    - Service `postgres` using image `pgvector/pgvector:pg17`
    - Container name: `ghostwriter-vectordb`
    - Environment: POSTGRES_USER=ghostwriter, POSTGRES_PASSWORD loaded from .env (VECTORDB_PASSWORD, default dev_password), POSTGRES_DB=knowledge_base
    - Port mapping: 5432:5432
    - Volume: `pgvector_data:/var/lib/postgresql/data` for persistence
    - Volume: `./execution/vector_db/init.sql:/docker-entrypoint-initdb.d/01-init.sql` for auto-init
    - `shm_size: 4gb` for HNSW index builds
    - Health check: `pg_isready -U ghostwriter -d knowledge_base` with interval 10s, timeout 5s, retries 5

    Create `execution/vector_db/init.sql` with:
    - `CREATE EXTENSION IF NOT EXISTS vector;`
    - Comment explaining this runs on first container start only
    - No table creation here (SQLAlchemy handles that via models.py)

    Do NOT modify the existing SQLite database or any existing source files.
  </action>
  <verify>
    Run `docker compose config` to validate docker-compose.yml syntax.
    Verify init.sql exists and contains the vector extension creation.
  </verify>
  <done>
    docker-compose.yml validates without errors. init.sql creates pgvector extension. No existing files modified.
  </done>
</task>

<task type="auto">
  <name>Task 2: SQLAlchemy models, connection management, and config</name>
  <files>
    execution/vector_db/__init__.py
    execution/vector_db/models.py
    execution/vector_db/connection.py
    execution/config.py
    requirements.txt
  </files>
  <action>
    **Create `execution/vector_db/__init__.py`:**
    - Module docstring explaining this is the vector database subsystem for the knowledge layer
    - Export key classes: Base, KnowledgeChunk, Document, IngestionLog, get_engine, get_session, init_db

    **Create `execution/vector_db/models.py`:**
    Use SQLAlchemy 2.0 declarative style with a NEW Base (do NOT reuse the SQLite metadata from sources/database.py -- these are separate databases).

    Define three models:

    1. `Document` -- represents a source document (email, paper, RSS item) before chunking:
       - id: Integer, primary key, autoincrement
       - tenant_id: String(50), nullable=False, default='default', indexed
       - source_type: String(50), nullable=False (email, rss, paper, manual)
       - source_id: String(255), nullable=False (unique per tenant+source_type)
       - title: Text, nullable=False
       - content: Text, nullable=False (full original text)
       - url: Text, nullable=True
       - date_published: DateTime, nullable=True
       - date_ingested: DateTime, default=datetime.utcnow
       - metadata_: JSONB, default={} (use metadata_ to avoid Python reserved word conflict; map to column name 'metadata' via Column('metadata', JSONB))
       - processing_status: String(20), default='pending' (pending, chunked, embedded, failed)
       - error_message: Text, nullable=True
       - UniqueConstraint on (tenant_id, source_type, source_id)

    2. `KnowledgeChunk` -- an embedded chunk from a document:
       - id: Integer, primary key, autoincrement
       - tenant_id: String(50), nullable=False, default='default', indexed
       - document_id: Integer, ForeignKey('documents.id', ondelete='CASCADE'), nullable=False
       - chunk_index: Integer, nullable=False (order within document)
       - content: Text, nullable=False (chunk text)
       - embedding: Vector(1536), nullable=True (null until embedded)
       - topic_tags: JSONB, default=[] (AI-generated tags)
       - entities: JSONB, default=[] (AI-extracted entities: [{type, value}])
       - cited_by: JSONB, default=[] (document IDs citing this chunk)
       - related_to: JSONB, default=[] (related chunk IDs)
       - created_at: DateTime, default=datetime.utcnow
       - updated_at: DateTime, default=datetime.utcnow, onupdate=datetime.utcnow
       - Index on (tenant_id, document_id)

    3. `IngestionLog` -- tracks ingestion runs for observability:
       - id: Integer, primary key, autoincrement
       - tenant_id: String(50), nullable=False, default='default'
       - started_at: DateTime, default=datetime.utcnow
       - completed_at: DateTime, nullable=True
       - status: String(20), default='running' (running, completed, failed, token_limit)
       - documents_processed: Integer, default=0
       - chunks_created: Integer, default=0
       - embeddings_generated: Integer, default=0
       - tokens_used: Integer, default=0
       - errors: JSONB, default=[]
       - source_type: String(50), nullable=True (null = all sources)

    Use `from pgvector.sqlalchemy import Vector` for the embedding column. Import JSONB from `sqlalchemy.dialects.postgresql`.

    Add relationship: Document.chunks = relationship('KnowledgeChunk', back_populates='document', cascade='all, delete-orphan'). KnowledgeChunk.document = relationship('Document', back_populates='chunks').

    **Create `execution/vector_db/connection.py`:**
    - `get_engine(db_url=None)` -- Creates SQLAlchemy engine. Reads URL from config if not provided. Uses create_engine with pool_pre_ping=True, pool_size=5, max_overflow=10. Singleton pattern (module-level _engine variable).
    - `get_session()` -- Returns a context manager that yields a Session (using sessionmaker bound to engine). NOT async for now (sync SQLAlchemy is fine, matches existing codebase pattern in sources/database.py).
    - `init_db()` -- Calls Base.metadata.create_all(engine) to create tables. Idempotent.
    - `reset_engine()` -- For testing, disposes and resets singleton.

    **Modify `execution/config.py`:**
    Add a new `VectorDBConfig` section to GhostWriterConfig:
    ```python
    class VectorDBConfig(BaseSettings):
        """Vector database configuration."""
        DATABASE_URL: str = "postgresql+psycopg://ghostwriter:dev_password@localhost:5432/knowledge_base"
        EMBEDDING_MODEL: str = "text-embedding-3-small"
        EMBEDDING_DIMENSIONS: int = 1536
        DAILY_TOKEN_LIMIT: int = 500_000  # embedding tokens per day
        DEFAULT_TENANT: str = "default"
        CHUNK_OVERLAP_PERCENT: float = 0.15  # 15% overlap between chunks
        # Chunk sizes per content type (tokens)
        CHUNK_SIZE_EMAIL: int = 400
        CHUNK_SIZE_PAPER: int = 512
        CHUNK_SIZE_RSS: int = 256
        CHUNK_SIZE_DEFAULT: int = 400

        model_config = SettingsConfigDict(
            env_file=".env",
            env_file_encoding="utf-8",
            extra="ignore",
            env_prefix="VECTORDB_",
        )
    ```
    Add `vector_db: VectorDBConfig = Field(default_factory=VectorDBConfig)` to `GhostWriterConfig`.
    Do NOT remove or modify any existing config sections.

    **Update `requirements.txt`:**
    Add these dependencies (append, do not remove existing):
    - pgvector
    - psycopg[binary]
    - (openai should already be present -- verify, add if missing)
    - apscheduler

    Follow existing codebase conventions:
    - Module docstrings on every file
    - Google-style docstrings on classes and public methods
    - Type hints on all function signatures
    - Snake case for functions/variables, PascalCase for classes
  </action>
  <verify>
    Run `python -c "from execution.vector_db.models import Base, KnowledgeChunk, Document, IngestionLog; print('Models OK')"` to verify imports.
    Run `python -c "from execution.vector_db.connection import get_engine, get_session, init_db; print('Connection OK')"` to verify connection module.
    Run `python -c "from execution.config import config; print(config.vector_db.DATABASE_URL)"` to verify config integration.
    Verify existing config still works: `python -c "from execution.config import config; print(config.api.OPENAI_API_KEY is not None or 'No key set')"`.
  </verify>
  <done>
    All vector_db module imports succeed. Config.vector_db section accessible with correct defaults. Existing config unchanged. requirements.txt updated with new dependencies.
  </done>
</task>

</tasks>

<verification>
1. `docker compose config` validates without errors
2. All Python imports from execution.vector_db succeed
3. Config singleton includes vector_db section with sensible defaults
4. Existing execution.sources.database module still works (no import conflicts)
5. No files outside of docker-compose.yml, execution/vector_db/, execution/config.py, and requirements.txt were modified
</verification>

<success_criteria>
- docker-compose.yml defines pgvector PostgreSQL with health check and persistent volume
- init.sql enables pgvector extension
- SQLAlchemy models define Document, KnowledgeChunk, and IngestionLog with tenant_id on all tables
- KnowledgeChunk has Vector(1536) embedding column
- Connection module provides engine and session management
- VectorDBConfig added to GhostWriterConfig with environment variable support
- requirements.txt includes pgvector and psycopg dependencies
- Zero impact on existing SQLite database layer
</success_criteria>

<output>
After completion, create `.planning/phases/01-vector-db-foundation/01-01-SUMMARY.md`
</output>
