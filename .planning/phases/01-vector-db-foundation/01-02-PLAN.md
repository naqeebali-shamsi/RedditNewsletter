---
phase: 01-vector-db-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - execution/vector_db/embeddings.py
  - execution/vector_db/token_tracking.py
autonomous: true

must_haves:
  truths:
    - "Text can be converted to 1536-dim vectors via OpenAI text-embedding-3-small"
    - "Batch API mode provides 50% cost savings for non-urgent embedding jobs"
    - "Synchronous mode embeds small batches immediately for interactive use"
    - "Daily token usage is tracked and enforced with configurable limits"
    - "Cost guardrail stops embedding when daily limit is exceeded"
  artifacts:
    - path: "execution/vector_db/embeddings.py"
      provides: "Embedding client with sync and batch modes"
      exports: ["EmbeddingClient", "embed_texts", "batch_embed_texts"]
      min_lines: 100
    - path: "execution/vector_db/token_tracking.py"
      provides: "Token usage tracking and daily limit enforcement"
      exports: ["TokenTracker", "check_daily_limit", "record_usage"]
      min_lines: 50
  key_links:
    - from: "execution/vector_db/embeddings.py"
      to: "execution/config.py"
      via: "reads EMBEDDING_MODEL, DAILY_TOKEN_LIMIT from config.vector_db"
      pattern: "config\\.vector_db\\.(EMBEDDING_MODEL|DAILY_TOKEN_LIMIT)"
    - from: "execution/vector_db/embeddings.py"
      to: "execution/vector_db/token_tracking.py"
      via: "checks token budget before embedding"
      pattern: "TokenTracker|check_daily_limit"
    - from: "execution/vector_db/embeddings.py"
      to: "openai"
      via: "OpenAI client for embedding API calls"
      pattern: "openai\\.OpenAI|client\\.embeddings\\.create"
---

<objective>
Build the OpenAI embedding pipeline with both synchronous (interactive) and batch (cost-efficient) modes, plus token usage tracking with daily cost guardrails.

Purpose: Every piece of knowledge entering the system needs to be converted to vector embeddings for semantic search. This plan creates the embedding infrastructure that the ingestion orchestrator (Plan 04) will call. Cost guardrails prevent runaway API spending during bulk ingestion.

Output: EmbeddingClient supporting sync and batch embedding modes, TokenTracker for daily usage monitoring, and cost guardrail enforcement.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-vector-db-foundation/01-RESEARCH.md
@.planning/phases/01-vector-db-foundation/01-CONTEXT.md
@.planning/phases/01-vector-db-foundation/01-01-SUMMARY.md
@execution/config.py
@execution/vector_db/models.py
@execution/vector_db/connection.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Token tracking and daily limit enforcement</name>
  <files>execution/vector_db/token_tracking.py</files>
  <action>
    Create `execution/vector_db/token_tracking.py` with:

    **Class `TokenTracker`:**
    - Tracks daily embedding token usage using a simple JSON file stored at `{TEMP_DIR}/token_usage.json`
    - JSON structure: `{"date": "YYYY-MM-DD", "tokens_used": int, "requests": int, "last_updated": "ISO timestamp"}`
    - If the date in the file doesn't match today, reset counters to zero (new day)

    **Methods:**
    - `__init__(self, daily_limit: int = None)` -- reads limit from config.vector_db.DAILY_TOKEN_LIMIT if not provided
    - `get_usage_today(self) -> dict` -- returns {"tokens_used": int, "requests": int, "remaining": int, "limit": int}
    - `check_budget(self, estimated_tokens: int) -> bool` -- returns True if estimated_tokens fits within remaining daily budget
    - `record_usage(self, tokens: int)` -- adds to today's counter, writes to file
    - `reset(self)` -- force reset counters (for testing)

    **Module-level functions (convenience):**
    - `check_daily_limit(estimated_tokens: int) -> bool` -- creates singleton TokenTracker, calls check_budget
    - `record_usage(tokens: int)` -- creates singleton TokenTracker, calls record_usage

    **Token estimation helper:**
    - `estimate_tokens(texts: list[str]) -> int` -- rough estimate at 1 token per 4 characters (standard approximation). Used to check budget before making API calls.

    Use pathlib for file paths. Handle concurrent access with basic file locking (try/except on write). Log warnings when daily limit is within 10% of being exceeded. Log errors when limit is exceeded and ingestion is blocked.

    Follow existing codebase patterns: module docstring, Google-style docstrings, type hints.
  </action>
  <verify>
    Run `python -c "from execution.vector_db.token_tracking import TokenTracker, check_daily_limit, estimate_tokens; t = TokenTracker(daily_limit=1000); print(t.get_usage_today()); print(estimate_tokens(['hello world'])); print(check_daily_limit(100))"` -- should show usage dict with 0 tokens, estimate ~3 tokens, and True.
  </verify>
  <done>
    TokenTracker reads/writes daily usage to JSON file. check_daily_limit returns True when under budget, False when exceeded. estimate_tokens provides reasonable estimates.
  </done>
</task>

<task type="auto">
  <name>Task 2: OpenAI embedding client with sync and batch modes</name>
  <files>execution/vector_db/embeddings.py</files>
  <action>
    Create `execution/vector_db/embeddings.py` with:

    **Class `EmbeddingClient`:**

    `__init__(self, model: str = None, api_key: str = None)`:
    - model defaults to config.vector_db.EMBEDDING_MODEL ("text-embedding-3-small")
    - api_key defaults to config.api.OPENAI_API_KEY
    - Creates openai.OpenAI client (synchronous -- matches existing codebase pattern, no async)
    - Stores a TokenTracker instance

    **Synchronous embedding (for small batches, interactive use):**
    `embed_texts(self, texts: list[str], check_budget: bool = True) -> list[list[float]]`:
    - Accepts up to 100 texts at a time (OpenAI limit)
    - If check_budget is True, calls self.tracker.check_budget() first; raises TokenBudgetExceeded if over limit
    - Calls `self.client.embeddings.create(model=self.model, input=texts)`
    - Records actual token usage from response.usage.total_tokens via self.tracker.record_usage()
    - Returns list of embedding vectors
    - Truncates any text longer than 8191 tokens (model limit) -- use first 8191*4 chars as rough cut
    - Retries on transient errors (rate limit, timeout) using tenacity: 3 attempts, exponential backoff. Match the pattern from base_agent.py.

    `embed_text(self, text: str) -> list[float]`:
    - Convenience wrapper, calls embed_texts([text])[0]

    **Batch embedding (for bulk ingestion, 50% cost savings):**
    `batch_embed_texts(self, texts: list[str], poll_interval: int = 30) -> list[list[float]]`:
    - Uses OpenAI Batch API for large-scale embedding
    - Creates JSONL batch file with requests (one per text)
    - Uploads file via client.files.create(purpose="batch")
    - Creates batch via client.batches.create(endpoint="/v1/embeddings", completion_window="24h")
    - Polls batch status every poll_interval seconds until completed/failed
    - On completion: downloads results, sorts by custom_id to maintain order, extracts embeddings
    - On failure: raises BatchEmbeddingFailed with error details
    - Records total tokens used after completion
    - Logs progress: "Batch {batch_id}: {status} ({n}/{total} complete)"

    **Custom exceptions (define at module level):**
    - `TokenBudgetExceeded(Exception)` -- daily token limit reached
    - `BatchEmbeddingFailed(Exception)` -- batch API returned failure
    - `EmbeddingError(Exception)` -- general embedding failure

    **Module-level convenience functions:**
    - `embed_texts(texts: list[str]) -> list[list[float]]` -- creates singleton EmbeddingClient, calls embed_texts
    - `batch_embed_texts(texts: list[str]) -> list[list[float]]` -- creates singleton EmbeddingClient, calls batch_embed_texts

    Important implementation notes:
    - Use `import openai` and `openai.OpenAI()` (synchronous client), NOT AsyncOpenAI. The existing codebase uses synchronous patterns.
    - For the batch API JSONL file, use a NamedTemporaryFile or write to config.paths.TEMP_DIR
    - The batch API JSONL format: one JSON object per line with keys custom_id, method, url, body
    - Log all API calls at INFO level (token count, batch status) for observability
    - The Batch API file.create() expects bytes, so encode the JSONL string
  </action>
  <verify>
    Run `python -c "from execution.vector_db.embeddings import EmbeddingClient, TokenBudgetExceeded, BatchEmbeddingFailed; print('Imports OK')"`.
    Run `python -c "from execution.vector_db.embeddings import EmbeddingClient; c = EmbeddingClient(); print(f'Model: {c.model}')"` -- should print "text-embedding-3-small".
    Note: actual embedding API calls require OPENAI_API_KEY and will cost money. Do NOT run live embedding tests in verification. Import and instantiation tests only.
  </verify>
  <done>
    EmbeddingClient instantiates with correct model from config. embed_texts() and batch_embed_texts() are implemented with retry logic and cost tracking. TokenBudgetExceeded raised when daily limit exceeded. Module-level convenience functions work.
  </done>
</task>

</tasks>

<verification>
1. All imports from execution.vector_db.embeddings succeed
2. All imports from execution.vector_db.token_tracking succeed
3. TokenTracker creates and reads usage file correctly
4. EmbeddingClient reads model name from config
5. No existing files modified (except __init__.py exports if needed)
</verification>

<success_criteria>
- EmbeddingClient supports both sync (embed_texts) and batch (batch_embed_texts) modes
- Token tracking persists daily usage to JSON file with automatic daily reset
- Cost guardrail blocks embedding when daily token limit exceeded
- Retry logic on transient API errors (rate limits, timeouts)
- Text truncation prevents exceeding OpenAI's 8191 token limit
- Batch API implementation handles file upload, polling, and result extraction
</success_criteria>

<output>
After completion, create `.planning/phases/01-vector-db-foundation/01-02-SUMMARY.md`
</output>
