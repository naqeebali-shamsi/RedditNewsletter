---
phase: 01-vector-db-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - execution/vector_db/chunking.py
  - execution/vector_db/tagging.py
autonomous: true

must_haves:
  truths:
    - "Text content is split into semantic chunks that preserve meaning"
    - "Chunk sizes vary by content type (email=400, paper=512, RSS=256 tokens)"
    - "Chunks overlap by 10-20% at boundaries to preserve context"
    - "AI auto-tagging extracts topic tags and named entities from content"
    - "Chunking works for email, paper, and RSS content types"
  artifacts:
    - path: "execution/vector_db/chunking.py"
      provides: "Semantic chunking pipeline with content-type strategies"
      exports: ["chunk_content", "SemanticChunker"]
      min_lines: 100
    - path: "execution/vector_db/tagging.py"
      provides: "AI-powered topic classification and entity extraction"
      exports: ["auto_tag", "AutoTagger"]
      min_lines: 60
  key_links:
    - from: "execution/vector_db/chunking.py"
      to: "execution/config.py"
      via: "reads chunk sizes from config.vector_db"
      pattern: "config\\.vector_db\\.CHUNK_SIZE"
    - from: "execution/vector_db/tagging.py"
      to: "execution/agents/base_agent.py"
      via: "uses LLM for structured extraction (inherits or uses BaseAgent pattern)"
      pattern: "call_llm|BaseAgent|openai|anthropic|google"
---

<objective>
Build the semantic chunking pipeline and AI auto-tagging system for processing ingested content into searchable knowledge.

Purpose: Raw documents (emails, papers, RSS items) need to be split into semantically meaningful chunks before embedding. Each chunk also needs topic tags and entity labels for metadata filtering in Phase 2. This plan creates both processing stages that the ingestion orchestrator (Plan 04) will call in sequence: chunk -> tag -> embed.

Output: SemanticChunker with content-type-aware splitting strategies, and AutoTagger for topic classification and entity extraction.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-vector-db-foundation/01-RESEARCH.md
@.planning/phases/01-vector-db-foundation/01-CONTEXT.md
@.planning/phases/01-vector-db-foundation/01-01-SUMMARY.md
@execution/config.py
@execution/vector_db/models.py
@execution/agents/base_agent.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Semantic chunking pipeline with content-type strategies</name>
  <files>execution/vector_db/chunking.py</files>
  <action>
    Create `execution/vector_db/chunking.py` with:

    **Dataclass `Chunk`:**
    - text: str (the chunk content)
    - chunk_index: int (order within document)
    - token_count: int (estimated token count)
    - topic_hint: str (optional topic label from boundary detection, default "")
    - metadata: dict (optional, default {})

    **Class `SemanticChunker`:**

    `__init__(self, default_chunk_size: int = None, overlap_percent: float = None)`:
    - Reads defaults from config.vector_db (CHUNK_SIZE_DEFAULT, CHUNK_OVERLAP_PERCENT)

    `chunk_content(self, content: str, content_type: str = "default") -> list[Chunk]`:
    - Main entry point. Routes to content-type-specific strategy:
      - "email" -> self._chunk_email(content)
      - "paper" -> self._chunk_paper(content)
      - "rss" -> self._chunk_rss(content)
      - "default" or anything else -> self._chunk_default(content)
    - Returns list of Chunk objects with sequential chunk_index values

    **Content-type strategies:**

    All strategies follow the same pattern but with different chunk sizes and boundary heuristics:

    `_chunk_email(self, content: str) -> list[Chunk]`:
    - Target: 400 tokens per chunk
    - Strip email headers/signatures first (look for common patterns: "---", "Unsubscribe", "View in browser")
    - Split on paragraph boundaries (double newlines) first
    - If any paragraph exceeds target size, split on sentence boundaries
    - Add overlap: include last 10-15% of previous chunk as prefix to next chunk

    `_chunk_paper(self, content: str) -> list[Chunk]`:
    - Target: 512 tokens per chunk
    - Respect section headers (lines starting with #, or ALL CAPS lines, or numbered sections)
    - Keep abstract as single chunk if under 512 tokens
    - Split body sections on paragraph boundaries
    - Add overlap at section boundaries

    `_chunk_rss(self, content: str) -> list[Chunk]`:
    - Target: 256 tokens per chunk
    - RSS items are typically short -- many will be a single chunk
    - If content exceeds target, split on paragraph then sentence boundaries
    - Minimal overlap (10%) since items are short

    `_chunk_default(self, content: str) -> list[Chunk]`:
    - Target: 400 tokens per chunk
    - Split on paragraph boundaries, then sentences if needed
    - Standard 15% overlap

    **Helper methods:**
    - `_split_into_sentences(self, text: str) -> list[str]` -- split text into sentences using regex (split on `. `, `! `, `? ` followed by capital letter or newline). Do NOT use nltk punkt tokenizer (heavy dependency for simple task).
    - `_estimate_tokens(self, text: str) -> int` -- 1 token per 4 characters approximation
    - `_merge_small_chunks(self, chunks: list[Chunk], min_tokens: int = 50) -> list[Chunk]` -- merge chunks smaller than min_tokens with adjacent chunks
    - `_add_overlap(self, chunks: list[Chunk], overlap_pct: float) -> list[Chunk]` -- prepend overlap_pct of previous chunk text to each chunk (except first)
    - `_strip_email_boilerplate(self, content: str) -> str` -- remove common email headers, footers, unsubscribe blocks

    **Module-level convenience function:**
    - `chunk_content(content: str, content_type: str = "default") -> list[Chunk]` -- creates SemanticChunker singleton, calls chunk_content

    **Design notes:**
    - This is a RULE-BASED chunker, not LLM-powered. The CONTEXT.md mentions LLM-powered chunking, but that is for newsletters specifically (Phase 3). For Phase 1, we build the foundational chunker with structural heuristics. LLM-powered boundary detection can be added as an enhancement on top of this in Phase 3 when we have actual newsletter content to test against.
    - Why rule-based first: LLM chunking costs money per call, adds latency, and we need a reliable fallback. The structural heuristics handle 90% of cases. LLM enhancement adds the remaining 10% for complex content.
    - The chunker must be deterministic and testable -- same input produces same output.
  </action>
  <verify>
    Run a quick test:
    ```python
    python -c "
    from execution.vector_db.chunking import chunk_content, Chunk
    # Test basic chunking
    text = 'First paragraph about AI.\n\nSecond paragraph about databases.\n\nThird paragraph about Python.'
    chunks = chunk_content(text, 'rss')
    print(f'Chunks: {len(chunks)}')
    for c in chunks:
        print(f'  [{c.chunk_index}] {c.token_count}t: {c.text[:50]}...')
    # Test email chunking
    email = 'View in browser\n\nMain content here about technology trends.\n\n---\nUnsubscribe'
    chunks = chunk_content(email, 'email')
    print(f'Email chunks: {len(chunks)}')
    assert all(isinstance(c, Chunk) for c in chunks)
    print('All tests passed')
    "
    ```
  </verify>
  <done>
    SemanticChunker produces Chunk objects with correct chunk_index ordering. Email boilerplate is stripped. Content-type-specific chunk sizes are applied. Overlap is added between chunks. Small chunks are merged.
  </done>
</task>

<task type="auto">
  <name>Task 2: AI auto-tagging with topic classification and entity extraction</name>
  <files>execution/vector_db/tagging.py</files>
  <action>
    Create `execution/vector_db/tagging.py` with:

    **Dataclass `TagResult`:**
    - topic_tags: list[str] (3-7 topic keywords like ["AI", "databases", "Python"])
    - entities: list[dict] (named entities: [{"type": "ORG", "value": "OpenAI"}, {"type": "TECH", "value": "PostgreSQL"}])
    - source_type_label: str (inferred content category: "newsletter", "research", "news", "tutorial", "opinion")
    - confidence: float (0-1, how confident the tagger is in its classification)

    **Class `AutoTagger`:**

    `__init__(self, model: str = None, provider: str = None)`:
    - model defaults to config.models.DEFAULT_FAST_MODEL (currently "llama-3.3-70b-versatile" via Groq -- cheap and fast)
    - Uses the existing BaseAgent pattern for LLM calls. Create a BaseAgent instance internally:
      ```python
      from execution.agents.base_agent import BaseAgent
      self._agent = BaseAgent(
          role="Content Tagger",
          persona="You are a content classification expert. Extract topics and entities from text.",
          model=model
      )
      ```
    - This reuses the existing multi-provider routing, retry logic, and error handling from BaseAgent.

    `tag_content(self, content: str, source_type: str = "unknown") -> TagResult`:
    - Sends content (truncated to first 4000 chars for speed/cost) to LLM with structured prompt
    - Prompt should request JSON output:
      ```
      Analyze this {source_type} content and extract:
      1. Topic tags: 3-7 keywords (e.g., AI, databases, Python, cloud computing)
      2. Named entities: organizations, people, technologies mentioned
      3. Content type: newsletter, research, news, tutorial, or opinion
      4. Confidence: 0-1 how confident you are

      Return ONLY valid JSON:
      {"topics": ["tag1", "tag2"], "entities": [{"type": "ORG|PERSON|TECH|CONCEPT", "value": "name"}], "content_type": "category", "confidence": 0.9}

      Content:
      {content[:4000]}
      ```
    - Parse JSON response. Use try/except for malformed JSON -- return empty TagResult on failure.
    - Do NOT use expect_json=True on BaseAgent.generate() -- instead call call_llm() and parse manually, because the response format may include markdown code fences that need stripping.

    `tag_batch(self, contents: list[tuple[str, str]]) -> list[TagResult]`:
    - Takes list of (content, source_type) tuples
    - Calls tag_content() sequentially for each item
    - Returns list of TagResults in same order
    - Logs progress: "Tagging {i}/{total}..."
    - Catches exceptions per-item (logs error, returns empty TagResult for that item, continues)

    **Module-level convenience function:**
    - `auto_tag(content: str, source_type: str = "unknown") -> TagResult` -- creates singleton AutoTagger, calls tag_content

    **Fallback behavior:**
    - If LLM call fails entirely (all providers exhausted), return TagResult with empty tags and confidence=0
    - Log the failure but don't crash -- tagging is enrichment, not critical path
    - If JSON parsing fails, try to extract what we can with regex (look for arrays in response text)
  </action>
  <verify>
    Run `python -c "from execution.vector_db.tagging import AutoTagger, TagResult, auto_tag; print('Imports OK'); t = AutoTagger(); print(f'Model: {t._agent.model}')"` -- should import and show model name.
    Note: actual LLM tagging calls require API keys. Import and instantiation tests only.
  </verify>
  <done>
    AutoTagger uses BaseAgent for LLM calls (reusing existing multi-provider routing). tag_content returns structured TagResult with topics, entities, source type label, and confidence. Graceful fallback on LLM failure. tag_batch processes items sequentially with per-item error isolation.
  </done>
</task>

</tasks>

<verification>
1. All imports from execution.vector_db.chunking succeed
2. All imports from execution.vector_db.tagging succeed
3. chunk_content produces correctly-ordered Chunk objects for all content types
4. Email boilerplate stripping works (removes "Unsubscribe", "View in browser")
5. AutoTagger instantiates with correct model from config
6. Existing BaseAgent and config modules unmodified
</verification>

<success_criteria>
- SemanticChunker handles email, paper, RSS, and default content types with appropriate chunk sizes
- Chunks include overlap at boundaries (10-20%)
- Small chunks are merged to avoid fragments
- AutoTagger extracts structured topics and entities via LLM
- Tagger gracefully handles LLM failures without crashing
- Both modules follow codebase conventions (docstrings, type hints, snake_case)
</success_criteria>

<output>
After completion, create `.planning/phases/01-vector-db-foundation/01-03-SUMMARY.md`
</output>
