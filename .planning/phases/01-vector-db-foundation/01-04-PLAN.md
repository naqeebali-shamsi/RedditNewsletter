---
phase: 01-vector-db-foundation
plan: 04
type: execute
wave: 3
depends_on: ["01-01", "01-02", "01-03"]
files_modified:
  - execution/vector_db/ingestion.py
  - execution/vector_db/indexing.py
  - execution/vector_db/__init__.py
  - scripts/test_vectordb.py
autonomous: false

must_haves:
  truths:
    - "Content flows through the full pipeline: document -> chunk -> tag -> embed -> store"
    - "Incremental re-indexing only processes documents changed since last run"
    - "Failed items are skipped with retry (3 attempts) and logged, not crashing the pipeline"
    - "HNSW index can be created on the embedding column for fast similarity search"
    - "A test script proves end-to-end: insert content, create embeddings, query by similarity"
  artifacts:
    - path: "execution/vector_db/ingestion.py"
      provides: "Ingestion orchestrator wiring chunking + embedding + tagging + storage"
      exports: ["IngestionPipeline", "ingest_document", "ingest_batch"]
      min_lines: 120
    - path: "execution/vector_db/indexing.py"
      provides: "HNSW index management and search utilities"
      exports: ["create_hnsw_index", "semantic_search"]
      min_lines: 60
    - path: "scripts/test_vectordb.py"
      provides: "End-to-end integration test script"
      min_lines: 50
  key_links:
    - from: "execution/vector_db/ingestion.py"
      to: "execution/vector_db/chunking.py"
      via: "chunks documents before embedding"
      pattern: "chunk_content|SemanticChunker"
    - from: "execution/vector_db/ingestion.py"
      to: "execution/vector_db/embeddings.py"
      via: "embeds chunks via EmbeddingClient"
      pattern: "EmbeddingClient|embed_texts"
    - from: "execution/vector_db/ingestion.py"
      to: "execution/vector_db/tagging.py"
      via: "tags content for metadata enrichment"
      pattern: "auto_tag|AutoTagger"
    - from: "execution/vector_db/ingestion.py"
      to: "execution/vector_db/models.py"
      via: "stores Documents and KnowledgeChunks in database"
      pattern: "Document|KnowledgeChunk|IngestionLog"
    - from: "execution/vector_db/ingestion.py"
      to: "execution/vector_db/connection.py"
      via: "gets database sessions for persistence"
      pattern: "get_session|get_engine"
    - from: "execution/vector_db/indexing.py"
      to: "execution/vector_db/connection.py"
      via: "executes raw SQL for index creation and search"
      pattern: "get_engine|engine\\.execute|text\\("
---

<objective>
Wire the complete ingestion pipeline (chunk -> tag -> embed -> store) with incremental re-indexing and HNSW index management, then verify the full system works end-to-end.

Purpose: This is the culmination of Phase 1 -- connecting all components into a working knowledge ingestion pipeline. After this plan, GhostWriter can accept any text content, break it into semantic chunks, tag it with topics/entities, generate embeddings, store everything in pgvector, and query by semantic similarity. The verification checkpoint confirms the entire stack works from Docker to query results.

Output: IngestionPipeline orchestrator, HNSW index management, and a proven end-to-end integration test.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-vector-db-foundation/01-RESEARCH.md
@.planning/phases/01-vector-db-foundation/01-CONTEXT.md
@.planning/phases/01-vector-db-foundation/01-01-SUMMARY.md
@.planning/phases/01-vector-db-foundation/01-02-SUMMARY.md
@.planning/phases/01-vector-db-foundation/01-03-SUMMARY.md
@execution/vector_db/models.py
@execution/vector_db/connection.py
@execution/vector_db/embeddings.py
@execution/vector_db/chunking.py
@execution/vector_db/tagging.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Ingestion orchestrator with incremental processing</name>
  <files>execution/vector_db/ingestion.py</files>
  <action>
    Create `execution/vector_db/ingestion.py` with:

    **Class `IngestionPipeline`:**

    `__init__(self, tenant_id: str = None)`:
    - tenant_id defaults to config.vector_db.DEFAULT_TENANT
    - Creates instances: SemanticChunker, EmbeddingClient, AutoTagger
    - Stores reference to connection module for session management

    `ingest_document(self, title: str, content: str, source_type: str, source_id: str, url: str = None, date_published: datetime = None, metadata: dict = None) -> Document`:
    - Full pipeline for a single document:
      1. Check if document already exists (by tenant_id + source_type + source_id). If exists and processing_status == 'embedded', skip (return existing). If exists and status is 'pending' or 'failed', delete existing chunks and reprocess.
      2. Create Document record with processing_status='pending'
      3. Chunk the content: `chunks = self.chunker.chunk_content(content, source_type)`
      4. Tag the full content (not each chunk -- tag once per document for efficiency): `tags = self.tagger.tag_content(content, source_type)`
      5. Create KnowledgeChunk records for each chunk, copying tags from document-level tagging
      6. Embed all chunk texts: `embeddings = self.embedder.embed_texts([c.text for c in chunks])`. Use sync mode (not batch) for individual document ingestion.
      7. Update each KnowledgeChunk record with its embedding vector
      8. Update Document.processing_status to 'embedded'
      9. Return the Document object

    - Error handling at each step:
      - Chunking fails -> set status='failed', error_message, skip to next
      - Tagging fails -> log warning, continue with empty tags (tagging is non-critical)
      - Embedding fails (TokenBudgetExceeded) -> set status='pending' (will retry tomorrow), log warning
      - Embedding fails (other) -> retry 3 times with 5s backoff, then set status='failed'
      - Database write fails -> rollback transaction, set status='failed', raise

    `ingest_batch(self, documents: list[dict], use_batch_api: bool = False) -> dict`:
    - Processes multiple documents. Each dict has keys matching ingest_document params.
    - Returns summary: {"processed": int, "skipped": int, "failed": int, "errors": list[str]}
    - Creates an IngestionLog record at start, updates it as processing proceeds
    - If use_batch_api is True:
      1. First pass: chunk and tag all documents, create DB records
      2. Collect all chunk texts
      3. Embed all at once via self.embedder.batch_embed_texts()
      4. Update all chunks with embeddings
      This is more efficient for large batches but has higher latency (10-20 min for batch API)
    - If use_batch_api is False:
      Process documents sequentially via ingest_document()
    - Per-item error isolation: if one document fails, log error and continue to next

    `get_pending_documents(self, since: datetime = None) -> list[Document]`:
    - Returns documents with processing_status in ('pending', 'failed')
    - If since provided, only documents created after that timestamp
    - Used by incremental re-indexing

    `reprocess_failed(self) -> dict`:
    - Re-runs ingestion for all documents with status='failed'
    - Returns same summary dict as ingest_batch

    **Module-level convenience functions:**
    - `ingest_document(**kwargs) -> Document` -- creates IngestionPipeline, calls ingest_document
    - `ingest_batch(documents: list[dict], **kwargs) -> dict` -- creates IngestionPipeline, calls ingest_batch

    **Logging:**
    - Log at INFO level: document title, chunk count, embedding token count
    - Log at WARNING level: tagging failures, token budget exceeded
    - Log at ERROR level: embedding failures, database errors
    - Use print() for logging (matching existing codebase pattern -- no structlog configured)

    **Transaction management:**
    - Use get_session() context manager for each document
    - Commit after each successful document (not after entire batch -- partial progress is better than all-or-nothing)
  </action>
  <verify>
    Run `python -c "from execution.vector_db.ingestion import IngestionPipeline, ingest_document, ingest_batch; print('Imports OK'); p = IngestionPipeline(); print(f'Tenant: {p.tenant_id}')"` -- should show 'default' tenant.
  </verify>
  <done>
    IngestionPipeline wires chunk -> tag -> embed -> store for single documents and batches. Duplicate detection skips already-processed documents. Per-item error isolation prevents one failure from crashing batch. IngestionLog tracks run statistics.
  </done>
</task>

<task type="auto">
  <name>Task 2: HNSW index management and semantic search</name>
  <files>
    execution/vector_db/indexing.py
    execution/vector_db/__init__.py
  </files>
  <action>
    **Create `execution/vector_db/indexing.py`:**

    `create_hnsw_index(m: int = 16, ef_construction: int = 64) -> bool`:
    - Creates HNSW index on knowledge_chunks.embedding column
    - Uses raw SQL via engine.execute():
      ```sql
      CREATE INDEX IF NOT EXISTS idx_chunks_hnsw
      ON knowledge_chunks
      USING hnsw (embedding vector_cosine_ops)
      WITH (m = {m}, ef_construction = {ef_construction});
      ```
    - Note: Uses vector_cosine_ops (not halfvec_cosine_ops) since we're storing full precision in the Vector(1536) column. halfvec optimization can be added later as a migration.
    - Sets maintenance_work_mem to '4GB' before index creation (via SET command)
    - Returns True on success, False on failure
    - Logs timing: "HNSW index created in {seconds}s"
    - Catches exceptions gracefully (index may already exist)

    `drop_hnsw_index() -> bool`:
    - Drops the HNSW index if it exists
    - Used for rebuilding index with different parameters

    `semantic_search(query_text: str, tenant_id: str = None, limit: int = 10, source_type: str = None) -> list[dict]`:
    - End-to-end semantic search:
      1. Embed the query text using EmbeddingClient.embed_text()
      2. Query pgvector for nearest neighbors:
         ```sql
         SELECT kc.id, kc.content, kc.topic_tags, kc.entities,
                d.title, d.source_type, d.url,
                kc.embedding <=> :query_embedding AS distance
         FROM knowledge_chunks kc
         JOIN documents d ON kc.document_id = d.id
         WHERE kc.tenant_id = :tenant_id
         {AND d.source_type = :source_type  -- if provided}
         ORDER BY kc.embedding <=> :query_embedding
         LIMIT :limit
         ```
      3. Return list of dicts: {"id", "content", "distance", "title", "source_type", "url", "topic_tags", "entities"}
    - tenant_id defaults to config.vector_db.DEFAULT_TENANT
    - Uses cosine distance operator `<=>`

    `get_index_stats() -> dict`:
    - Returns stats about the HNSW index: {"exists": bool, "size_bytes": int, "rows_indexed": int}
    - Uses pg_indexes and pg_relation_size system views

    **Update `execution/vector_db/__init__.py`:**
    - Add all new exports from Plans 01-04:
      - models: Base, Document, KnowledgeChunk, IngestionLog
      - connection: get_engine, get_session, init_db
      - embeddings: EmbeddingClient, embed_texts, batch_embed_texts
      - chunking: chunk_content, SemanticChunker, Chunk
      - tagging: auto_tag, AutoTagger, TagResult
      - ingestion: IngestionPipeline, ingest_document, ingest_batch
      - indexing: create_hnsw_index, semantic_search, get_index_stats
    - Use __all__ list for explicit exports
  </action>
  <verify>
    Run `python -c "from execution.vector_db.indexing import create_hnsw_index, semantic_search, get_index_stats; print('Imports OK')"`.
    Run `python -c "from execution.vector_db import IngestionPipeline, semantic_search, chunk_content, auto_tag; print('All imports from package OK')"`.
  </verify>
  <done>
    HNSW index can be created with configurable parameters. semantic_search embeds query and returns ranked results with document metadata. Package __init__.py exports all public APIs.
  </done>
</task>

<task type="auto">
  <name>Task 3: End-to-end integration test script</name>
  <files>scripts/test_vectordb.py</files>
  <action>
    Create `scripts/test_vectordb.py` -- a standalone script that exercises the full pipeline.

    **Prerequisites check:**
    - Verify Docker is running: `docker ps` and look for ghostwriter-vectordb
    - If not running, print instructions and exit: "Run 'docker compose up -d' first"
    - Verify OPENAI_API_KEY is set (needed for embeddings)
    - If not set, print instructions and exit

    **Test sequence:**

    1. **Database setup:**
       - Call init_db() to create tables
       - Print table names from Base.metadata.tables

    2. **Single document ingestion:**
       - Create a test document:
         ```python
         doc = ingest_document(
             title="Test: PostgreSQL Vector Search in 2026",
             content="""PostgreSQL with pgvector has become the standard approach for vector
             similarity search in 2026. The HNSW indexing algorithm provides fast approximate
             nearest neighbor search, while halfvec storage cuts memory usage by 50%.

             OpenAI's text-embedding-3-small model generates 1536-dimensional vectors at
             $0.01 per million tokens via the Batch API. This makes semantic search affordable
             even for large knowledge bases.

             Key benefits include: native SQL integration, ACID compliance, and the ability
             to combine vector search with traditional filtering on metadata columns like
             source_type and tenant_id.""",
             source_type="rss",
             source_id="test-001",
             url="https://example.com/test"
         )
         ```
       - Print: document ID, chunk count, processing status

    3. **Ingest a second document (different topic):**
       - Title: "Remote Work Trends for Digital Nomads"
       - Content about remote work, coworking spaces, visa programs
       - source_type="email", source_id="test-002"

    4. **Verify duplicate detection:**
       - Try to ingest test-001 again
       - Should skip (not create duplicate chunks)

    5. **Create HNSW index:**
       - Call create_hnsw_index()
       - Print index stats

    6. **Semantic search:**
       - Query: "How does pgvector handle vector indexing?"
       - Should return the PostgreSQL document with higher relevance
       - Print results with distances
       - Query: "What are the best cities for remote workers?"
       - Should return the remote work document
       - Print results with distances

    7. **Cleanup option:**
       - If --cleanup flag passed, delete test documents and chunks
       - Otherwise leave them for manual inspection

    **Script should be runnable as:**
    ```bash
    python scripts/test_vectordb.py
    python scripts/test_vectordb.py --cleanup
    ```

    Use argparse for the --cleanup flag. Print clear section headers and results.
    Exit with code 0 on success, 1 on failure.
  </action>
  <verify>
    Run `python -c "import scripts.test_vectordb; print('Script imports OK')"` or just verify the file exists and has valid Python syntax: `python -m py_compile scripts/test_vectordb.py`.
    Note: Actually running the script requires Docker + OPENAI_API_KEY. That's verified in the checkpoint task below.
  </verify>
  <done>
    Test script validates full pipeline: init -> ingest -> chunk -> embed -> index -> search. Can run standalone with clear pass/fail output.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
    Complete vector database foundation:
    - Docker PostgreSQL with pgvector extension
    - SQLAlchemy models (Document, KnowledgeChunk, IngestionLog)
    - OpenAI embedding pipeline (sync + batch modes)
    - Semantic chunking (email, paper, RSS strategies)
    - AI auto-tagging (topics + entities)
    - Ingestion orchestrator (chunk -> tag -> embed -> store)
    - HNSW index creation and semantic search
    - End-to-end integration test script
  </what-built>
  <how-to-verify>
    1. Ensure Docker Desktop is running
    2. Start the database: `docker compose up -d`
    3. Wait for health check: `docker compose ps` should show "healthy"
    4. Ensure OPENAI_API_KEY is in .env
    5. Install new dependencies: `pip install pgvector psycopg[binary] apscheduler`
    6. Run the integration test: `python scripts/test_vectordb.py`
    7. Verify output shows:
       - Tables created successfully
       - Test document 1 ingested with N chunks
       - Test document 2 ingested with N chunks
       - Duplicate skipped
       - HNSW index created
       - Search for "pgvector" returns document 1 with low distance
       - Search for "remote work" returns document 2 with low distance
    8. Optionally clean up: `python scripts/test_vectordb.py --cleanup`
  </how-to-verify>
  <resume-signal>Type "approved" if integration test passes, or describe any issues</resume-signal>
</task>

</tasks>

<verification>
1. Docker container starts and pgvector extension is available
2. All tables created via SQLAlchemy models
3. Documents can be ingested through the full pipeline
4. Duplicate documents are detected and skipped
5. Semantic search returns relevant results ordered by distance
6. HNSW index created successfully
7. Existing SQLite pipeline unaffected (reddit_content.db still works)
</verification>

<success_criteria>
- End-to-end test passes: content goes in, embeddings come out, search returns relevant results
- Ingestion handles failures gracefully (per-item isolation, retry on transient errors)
- HNSW index created with m=16, ef_construction=64
- Semantic search returns results with cosine distance scores
- No modifications to existing pipeline files (sources/, agents/, pipeline.py)
- Phase 1 success criterion met: "PostgreSQL with pgvector stores and queries vector embeddings with HNSW indexing"
</success_criteria>

<output>
After completion, create `.planning/phases/01-vector-db-foundation/01-04-SUMMARY.md`
</output>
