---
phase: 02-retrieval-tools
plan: "02"
type: execute
wave: 1
depends_on: []
files_modified:
  - execution/vector_db/bm25_index.py
  - execution/vector_db/reranking.py
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "BM25 index can be built from knowledge_chunks corpus and searched by keyword query"
    - "CrossEncoder reranks retrieval candidates by query-document relevance with configurable model profiles"
    - "New dependencies (bm25s, sentence-transformers) are installed and importable"
  artifacts:
    - path: "execution/vector_db/bm25_index.py"
      provides: "BM25 sparse retrieval using bm25s library"
      exports: ["BM25Index"]
    - path: "execution/vector_db/reranking.py"
      provides: "CrossEncoder reranking with sentence-transformers"
      exports: ["CrossEncoderReranker"]
    - path: "requirements.txt"
      provides: "Updated dependencies with bm25s and sentence-transformers"
      contains: "bm25s"
  key_links:
    - from: "execution/vector_db/bm25_index.py"
      to: "execution/vector_db/models.py"
      via: "SQLAlchemy query for corpus loading"
      pattern: "KnowledgeChunk.*content"
    - from: "execution/vector_db/bm25_index.py"
      to: "bm25s"
      via: "BM25 indexing and retrieval"
      pattern: "bm25s\\.BM25|bm25s\\.tokenize"
    - from: "execution/vector_db/reranking.py"
      to: "sentence_transformers"
      via: "CrossEncoder model loading"
      pattern: "CrossEncoder"
---

<objective>
Build the BM25 sparse retrieval index and CrossEncoder reranking modules that provide the two key capabilities beyond basic vector search: keyword matching and precision reranking.

Purpose: BM25 captures exact keyword matches that semantic search misses (proper nouns, technical terms, acronyms). CrossEncoder reranking improves precision by scoring query-document pairs with a fine-grained model, reducing noise in the final top-K results. Together with Plan 02-01's utilities, these modules give the hybrid retrieval orchestrator (Plan 02-03) all the building blocks it needs.

Output: Two new Python modules + updated requirements.txt with bm25s and sentence-transformers dependencies.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-retrieval-tools/02-RESEARCH.md

# Existing code this plan builds on
@execution/vector_db/models.py
@execution/vector_db/connection.py
@execution/vector_db/indexing.py
@execution/config.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Dependencies and BM25 Index Module</name>
  <files>execution/vector_db/bm25_index.py, requirements.txt</files>
  <action>
**Step 1: Install new dependencies**

Run: `pip install bm25s sentence-transformers PyStemmer`

Then update `requirements.txt` — add these lines in a new section after the existing "Token Counting" section:

```
# Hybrid Search (BM25 + Reranking)
bm25s>=0.2.0
sentence-transformers>=3.0.0
PyStemmer>=2.2.0
```

**Step 2: Create `execution/vector_db/bm25_index.py`**

Implement a `BM25Index` class that manages a bm25s-based sparse index for lexical retrieval.

**BM25Index class:**

1. `__init__(self, index_dir: Optional[str] = None)` — Store index directory path (default: `.tmp/bm25_index/`). Initialize `self.retriever = None`, `self.chunk_ids = []`, `self.is_loaded = False`. Use `execution.config.config.paths.TEMP_DIR / "bm25_index"` as default path.

2. `build_index(self, tenant_id: str = "default") -> int` — Build BM25 index from all knowledge_chunks for the given tenant.
   - Query all KnowledgeChunk rows where `tenant_id == tenant_id` and `content IS NOT NULL`
   - Extract corpus: `[chunk.content for chunk in chunks]`
   - Store chunk IDs: `[chunk.id for chunk in chunks]`
   - Tokenize with `bm25s.tokenize(corpus, stopwords="en")`
   - Create `bm25s.BM25()` retriever and call `.index(corpus_tokens)`
   - Save to disk: `self.retriever.save(str(self.index_dir), corpus=corpus_tokens)`
   - Also save chunk_ids to a JSON file alongside: `self.index_dir / "chunk_ids.json"`
   - Set `self.is_loaded = True`
   - Return number of documents indexed
   - Log info: "BM25 index built: {N} chunks for tenant {tenant_id}"

3. `load_index(self) -> bool` — Load saved index from disk.
   - Try `bm25s.BM25.load(str(self.index_dir), load_corpus=False)`
   - Load chunk_ids from JSON file
   - Set `self.is_loaded = True`
   - Return True on success, False on FileNotFoundError
   - Log info on success, warning on failure

4. `search(self, query: str, top_k: int = 50) -> List[Dict]` — Search BM25 index.
   - If not loaded, try `load_index()`. If still not loaded, return empty list with warning.
   - Tokenize query with `bm25s.tokenize(query, stopwords="en")`
   - Retrieve: `results, scores = self.retriever.retrieve(query_tokens, k=min(top_k, len(self.chunk_ids)))`
   - Return list of dicts: `{"id": chunk_id, "bm25_score": float(score), "rank": i+1}`
   - Handle edge case where top_k > corpus size (bm25s requires k <= corpus size)
   - Filter out results with score <= 0 (no match)

5. `needs_rebuild(self, tenant_id: str = "default") -> bool` — Check if index is stale.
   - Compare chunk count in DB vs saved index size
   - Return True if counts differ or index doesn't exist

6. `clear_index(self) -> None` — Delete saved index files from disk. Reset state.

**Important patterns:**
- Use `get_session()` context manager from connection.py for all DB queries
- Use `import json` for chunk_ids serialization
- Use `import logging` with `logger = logging.getLogger(__name__)`
- Use `pathlib.Path` for index_dir
- Handle the bm25s API correctly: `tokenize()` returns list of token lists, `retrieve()` returns (results_array, scores_array) where each is 2D (one row per query)
- The `retrieve()` method returns numpy arrays — index into `results[0]` and `scores[0]` for single query
- bm25s stores integer indices into the corpus, not chunk IDs — map via `self.chunk_ids[idx]`

**Do NOT:**
- Import or use EmbeddingClient (this is purely lexical search)
- Run index build on import (lazy loading only)
- Block on index build during search — return empty list if no index available
  </action>
  <verify>
Run: `pip install bm25s sentence-transformers PyStemmer` — installs without errors.

Run: `python -c "import bm25s; print(f'bm25s version: {bm25s.__version__}')"` — prints version.

Run: `python -c "
from execution.vector_db.bm25_index import BM25Index
idx = BM25Index()
print(f'Index dir: {idx.index_dir}')
print(f'Is loaded: {idx.is_loaded}')
# Search without index should return empty list gracefully
results = idx.search('test query')
print(f'Empty search results: {results}')
print('BM25Index import and basic usage OK')
"` — prints index dir, False, empty list, success message.

Verify requirements.txt has new entries: `python -c "print([l.strip() for l in open('requirements.txt') if 'bm25s' in l or 'sentence-transformers' in l])"` — shows both entries.
  </verify>
  <done>BM25Index class with build_index (from DB), load_index (from disk), search (keyword retrieval), needs_rebuild (staleness check), and clear_index. Persists to .tmp/bm25_index/. requirements.txt updated with bm25s, sentence-transformers, PyStemmer.</done>
</task>

<task type="auto">
  <name>Task 2: CrossEncoder Reranking Module</name>
  <files>execution/vector_db/reranking.py</files>
  <action>
Create `execution/vector_db/reranking.py` with a `CrossEncoderReranker` class that reranks retrieval candidates using sentence-transformers CrossEncoder models.

**CrossEncoderReranker class:**

1. Class-level `MODELS` dict mapping profile names to model identifiers:
   ```python
   MODELS = {
       "fast": "cross-encoder/ms-marco-TinyBERT-L2-v2",      # 9000 docs/sec, 69.84 NDCG
       "balanced": "cross-encoder/ms-marco-MiniLM-L6-v2",    # 1800 docs/sec, 74.30 NDCG
       "accurate": "cross-encoder/ms-marco-MiniLM-L12-v2",   # 960 docs/sec, 74.31 NDCG
   }
   ```

2. `__init__(self, model_profile: str = "balanced")` — Lazy model loading. Store `model_profile` and `model_name` but do NOT load the model yet. Set `self._model = None`. Model loads on first `rerank()` call. This avoids slow import/download on module import.

3. `_ensure_model(self)` — Internal method: load CrossEncoder model if not yet loaded. `from sentence_transformers import CrossEncoder; self._model = CrossEncoder(self.model_name)`. Log info with model name and profile. This is called lazily on first use.

4. `rerank(self, query: str, candidates: List[Dict], top_k: int = 10, batch_size: int = 32, timeout_ms: int = 5000) -> List[Dict]` — Rerank candidates by query relevance.
   - Call `_ensure_model()` to lazy-load
   - If candidates is empty, return empty list
   - Cap candidates at 100 (log warning if truncated): "Limiting reranking from {N} to 100 candidates"
   - Build pairs: `[(query, candidate["content"][:2000]) for candidate in candidates]` — truncate content to 2000 chars to control latency
   - Score with `self._model.predict(pairs, batch_size=batch_size)`
   - Add `rerank_score` (float) to each candidate dict
   - Sort by rerank_score descending
   - Return top_k results
   - Time the operation and log: "CrossEncoder reranked {N} candidates in {elapsed_ms:.0f}ms ({docs_per_sec:.0f} docs/sec)"
   - If elapsed > timeout_ms, log warning: "Reranking exceeded timeout ({elapsed_ms}ms > {timeout_ms}ms)"

5. `rank_passages(self, query: str, passages: List[str]) -> List[Dict]` — Simplified interface for ad-hoc ranking (not part of main retrieval pipeline). Takes raw passage strings, returns `[{"corpus_id": int, "score": float}, ...]` sorted by score descending. Useful for testing and one-off comparisons.

**Important patterns:**
- Use `import time` for latency measurement
- Use `import logging` with `logger = logging.getLogger(__name__)`
- Lazy loading is critical — `sentence_transformers` import is slow (~2-3 sec) and downloads models on first use
- The `predict()` method returns a numpy array of scores — convert to float with `float(score)`
- Candidates are List[Dict] where each dict MUST have "content" key (str). Other keys are preserved and passed through.
- Do NOT mutate the original candidate dicts — create copies with `candidate.copy()` before adding rerank_score? Actually, mutating is fine here since the retrieval pipeline produces fresh dicts each time. Just add the key.

**Do NOT:**
- Load model on import or __init__ (lazy only)
- Import sentence_transformers at module level (import inside _ensure_model)
- Remove or modify candidates' existing keys (only add rerank_score)
  </action>
  <verify>
Run: `python -c "
from execution.vector_db.reranking import CrossEncoderReranker
reranker = CrossEncoderReranker(model_profile='balanced')
print(f'Model profile: {reranker.model_profile}')
print(f'Model name: {reranker.model_name}')
print(f'Model loaded: {reranker._model is not None}')  # Should be False (lazy)
print('CrossEncoderReranker import OK (model NOT loaded yet)')
"` — prints profile, name, False, success.

Run: `python -c "
from execution.vector_db.reranking import CrossEncoderReranker
reranker = CrossEncoderReranker(model_profile='fast')  # Use fast for test speed
candidates = [
    {'id': 1, 'content': 'PostgreSQL vector search with pgvector uses HNSW indexing for fast approximate nearest neighbor queries.'},
    {'id': 2, 'content': 'The weather in London is rainy and cold most of the year.'},
    {'id': 3, 'content': 'Vector databases store high-dimensional embeddings for similarity search applications.'},
]
results = reranker.rerank('How does pgvector handle vector indexing?', candidates, top_k=3)
for r in results:
    print(f'  id={r[\"id\"]} rerank_score={r[\"rerank_score\"]:.4f} content={r[\"content\"][:60]}')
# ID 1 (pgvector) should rank highest, ID 2 (weather) should rank lowest
assert results[0]['id'] in [1, 3], 'Expected pgvector or vector DB doc to rank first'
assert results[-1]['id'] == 2, 'Expected weather doc to rank last'
print('Reranking test passed')
"` — model downloads on first run (may take 30-60 sec), then reranking produces correct ordering.
  </verify>
  <done>CrossEncoderReranker with lazy model loading, three model profiles (fast/balanced/accurate), rerank() with latency logging and timeout warnings, and rank_passages() for ad-hoc use. Caps candidates at 100, truncates content at 2000 chars.</done>
</task>

</tasks>

<verification>
Dependencies installed and importable:
```bash
python -c "import bm25s; import sentence_transformers; print('Dependencies OK')"
```

Both modules importable without side effects:
```bash
python -c "from execution.vector_db.bm25_index import BM25Index; print('BM25Index OK')"
python -c "from execution.vector_db.reranking import CrossEncoderReranker; print('Reranker OK')"
```

Existing Phase 1 integration test unaffected (no existing files modified except requirements.txt):
```bash
python -c "from execution.vector_db import semantic_search, create_hnsw_index; print('Phase 1 imports OK')"
```
</verification>

<success_criteria>
1. `pip install bm25s sentence-transformers PyStemmer` succeeds
2. requirements.txt contains bm25s, sentence-transformers, PyStemmer entries
3. BM25Index.build_index() creates index from DB corpus, saves to disk
4. BM25Index.search() returns ranked results with bm25_score and rank
5. BM25Index gracefully returns empty list when no index exists
6. CrossEncoderReranker lazy-loads model on first rerank() call (not on import)
7. Reranking correctly prioritizes relevant documents over irrelevant ones
8. Reranking logs latency and warns on timeout
9. Existing Phase 1 code unmodified and still importable
</success_criteria>

<output>
After completion, create `.planning/phases/02-retrieval-tools/02-02-SUMMARY.md`
</output>
