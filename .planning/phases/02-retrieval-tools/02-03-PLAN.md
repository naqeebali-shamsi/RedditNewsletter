---
phase: 02-retrieval-tools
plan: "03"
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - execution/vector_db/retrieval.py
  - execution/vector_db/__init__.py
  - execution/config.py
  - scripts/test_retrieval.py
autonomous: true

must_haves:
  truths:
    - "Hybrid search combines BM25 keyword matching + vector semantic search with RRF fusion into a single query interface"
    - "Metadata filters scope searches by date range, source type, and topic tags before retrieval"
    - "CrossEncoder reranking improves precision on fused results before returning top-K"
    - "Recency scoring automatically boosts recent documents for trend-sensitive queries"
    - "Fine-grained citations provide sentence-level source attribution with clickable markdown links"
    - "Existing semantic_search() function and Phase 1 integration test are not broken"
  artifacts:
    - path: "execution/vector_db/retrieval.py"
      provides: "Hybrid retrieval orchestrator composing all Phase 2 modules"
      exports: ["HybridRetriever", "RetrievalResult", "hybrid_search"]
    - path: "execution/vector_db/__init__.py"
      provides: "Updated package exports including all Phase 2 modules"
      contains: "HybridRetriever"
    - path: "execution/config.py"
      provides: "RetrievalConfig with tunable parameters"
      contains: "RetrievalConfig"
    - path: "scripts/test_retrieval.py"
      provides: "End-to-end retrieval integration test"
      contains: "test_hybrid_search"
  key_links:
    - from: "execution/vector_db/retrieval.py"
      to: "execution/vector_db/bm25_index.py"
      via: "BM25 search leg of hybrid retrieval"
      pattern: "BM25Index.*search"
    - from: "execution/vector_db/retrieval.py"
      to: "execution/vector_db/indexing.py"
      via: "Vector search leg (existing semantic_search)"
      pattern: "semantic_search"
    - from: "execution/vector_db/retrieval.py"
      to: "execution/vector_db/reranking.py"
      via: "CrossEncoder reranking of fused results"
      pattern: "CrossEncoderReranker.*rerank"
    - from: "execution/vector_db/retrieval.py"
      to: "execution/vector_db/metadata_filters.py"
      via: "Pre-retrieval filter building"
      pattern: "build_filters|MetadataFilter"
    - from: "execution/vector_db/retrieval.py"
      to: "execution/vector_db/recency_scoring.py"
      via: "Post-fusion recency boost for trend queries"
      pattern: "RecencyScorer.*score_results"
    - from: "execution/vector_db/retrieval.py"
      to: "execution/vector_db/citations.py"
      via: "Citation extraction from final results"
      pattern: "CitationExtractor.*extract"
---

<objective>
Build the hybrid retrieval orchestrator that composes all Phase 2 modules (BM25, vector search, RRF fusion, metadata filters, recency scoring, reranking, citations) into a single, clean retrieval interface. Plus update config, package exports, and create an integration test.

Purpose: This is the capstone of Phase 2 — the module that downstream agents (Phase 5) will actually call. It provides a `hybrid_search()` function that handles the full two-stage retrieval pipeline: BM25 + vector -> RRF fusion -> metadata filtering -> recency boost -> CrossEncoder reranking -> citation extraction.

Output: Hybrid retrieval orchestrator, updated config, updated package exports, integration test script.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-retrieval-tools/02-RESEARCH.md

# Plan 02-01 and 02-02 SUMMARYs (read these for what was actually built)
@.planning/phases/02-retrieval-tools/02-01-SUMMARY.md
@.planning/phases/02-retrieval-tools/02-02-SUMMARY.md

# All Phase 2 modules this plan orchestrates
@execution/vector_db/metadata_filters.py
@execution/vector_db/recency_scoring.py
@execution/vector_db/citations.py
@execution/vector_db/bm25_index.py
@execution/vector_db/reranking.py

# Existing code to extend (NOT replace)
@execution/vector_db/indexing.py
@execution/vector_db/models.py
@execution/vector_db/connection.py
@execution/vector_db/__init__.py
@execution/config.py
@scripts/test_vectordb.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Hybrid Retrieval Orchestrator + Config</name>
  <files>execution/vector_db/retrieval.py, execution/config.py</files>
  <action>
**Step 1: Add RetrievalConfig to execution/config.py**

Add a new `RetrievalConfig` class to `execution/config.py` (after `VectorDBConfig`, before `GhostWriterConfig`):

```python
class RetrievalConfig(BaseSettings):
    """Hybrid retrieval configuration."""

    # BM25 settings
    BM25_TOP_K: int = 50
    BM25_WEIGHT: float = 1.0

    # Vector search settings
    VECTOR_TOP_K: int = 50
    VECTOR_WEIGHT: float = 1.0

    # RRF fusion
    RRF_K: int = 60
    FUSION_TOP_K: int = 50

    # Reranking
    RERANK_ENABLED: bool = True
    RERANK_MODEL_PROFILE: str = "balanced"  # fast, balanced, accurate
    RERANK_TOP_K: int = 10
    RERANK_TIMEOUT_MS: int = 5000

    # Recency scoring
    RECENCY_HALF_LIFE_DAYS: int = 14
    RECENCY_SEMANTIC_WEIGHT: float = 0.7
    RECENCY_WEIGHT: float = 0.3

    # Final output
    DEFAULT_TOP_K: int = 10

    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
        env_prefix="RETRIEVAL_",
    )
```

Then add `retrieval: RetrievalConfig = Field(default_factory=RetrievalConfig)` to the `GhostWriterConfig` class (after `vector_db` field).

**Step 2: Create `execution/vector_db/retrieval.py`**

This is the main orchestrator module. Implement:

**RetrievalResult dataclass:**
```python
@dataclass
class RetrievalResult:
    id: int                          # chunk ID
    content: str                     # chunk text
    title: Optional[str]             # document title
    url: Optional[str]               # document URL
    source_type: Optional[str]       # email, rss, paper
    date_published: Optional[datetime]  # document date
    topic_tags: Optional[list]       # chunk topic tags
    entities: Optional[list]         # chunk entities

    # Scoring
    rrf_score: float = 0.0           # RRF fusion score (0-1 normalized)
    rerank_score: Optional[float] = None  # CrossEncoder score (if reranking enabled)
    recency_score: Optional[float] = None # Recency decay score (if trend query)
    fused_score: Optional[float] = None   # Final fused score (semantic + recency)

    # Citations (populated on demand)
    citations: Optional[list] = None  # List[Citation] from citations module
```

**HybridRetriever class:**

1. `__init__(self, config: Optional[RetrievalConfig] = None)` — Load config from `execution.config.config.retrieval` if not provided. Initialize `BM25Index`, `RecencyScorer(config.RECENCY_HALF_LIFE_DAYS)`. Do NOT initialize `CrossEncoderReranker` yet (lazy).

2. `_vector_search(self, query: str, tenant_id: str, top_k: int, filters: Optional[List] = None) -> List[Dict]` — Call existing `semantic_search()` from indexing.py for basic vector search. Then enhance: if `filters` is provided, build a custom SQL query that applies the SQLAlchemy filter conditions from metadata_filters BEFORE the vector distance ordering. This is critical for performance — filters must be in WHERE clause, not applied after retrieval.

   Implementation for filtered vector search:
   - Get query embedding via `EmbeddingClient().embed_text(query)`
   - Build SQLAlchemy select statement joining KnowledgeChunk + Document
   - Apply all filter conditions from `filters` list (these come from build_filters())
   - Order by `KnowledgeChunk.embedding.cosine_distance(query_embedding)`
   - Limit to `top_k`
   - Execute and return results as List[Dict] with keys: id, content, title, url, source_type, date_published, topic_tags, entities, distance
   - Add rank (1-based) to each result

   If no filters provided, delegate to existing `semantic_search()` and add date_published from a follow-up query or JOIN.

3. `_bm25_search(self, query: str, tenant_id: str, top_k: int) -> List[Dict]` — Delegate to `self.bm25_index.search(query, top_k)`. If BM25 index not loaded, try loading. If still not available, log warning and return empty list (graceful degradation to vector-only search).

4. `_rrf_fusion(self, bm25_results: List[Dict], vector_results: List[Dict], k: int = 60, bm25_weight: float = 1.0, vector_weight: float = 1.0) -> List[Dict]` — Implement Reciprocal Rank Fusion:
   - For each result from each source, compute: `contribution = weight / (k + rank)`
   - Accumulate scores by chunk ID
   - Merge metadata from both sources (prefer vector results which have more fields from the JOIN)
   - Normalize final RRF scores to 0-1 range (divide by max score)
   - Sort by RRF score descending
   - Return fused results with "rrf_score" key

5. `_enrich_bm25_results(self, bm25_results: List[Dict], tenant_id: str) -> List[Dict]` — BM25 results only have {id, bm25_score, rank}. Enrich with metadata by querying the DB: JOIN KnowledgeChunk + Document for the chunk IDs. Add content, title, url, source_type, date_published, topic_tags, entities. Do this in a single batch query for efficiency.

6. `search(self, query: str, tenant_id: str = "default", top_k: Optional[int] = None, source_types: Optional[List[str]] = None, topic_tags: Optional[List[str]] = None, date_range: Optional[Tuple[datetime, datetime]] = None, recency_months: Optional[int] = None, rerank: Optional[bool] = None, include_citations: bool = False) -> List[RetrievalResult]` — Main entry point. The full pipeline:

   a. Build metadata filters via `build_filters(tenant_id, date_range, source_types, topic_tags, recency_months=recency_months)`
   b. Run vector search with filters: `_vector_search(query, tenant_id, config.VECTOR_TOP_K, filters)`
   c. Run BM25 search: `_bm25_search(query, tenant_id, config.BM25_TOP_K)`
   d. Enrich BM25 results: `_enrich_bm25_results(bm25_results, tenant_id)`
   e. RRF fusion: `_rrf_fusion(bm25_results, vector_results, config.RRF_K, config.BM25_WEIGHT, config.VECTOR_WEIGHT)`
   f. Recency scoring: `self.recency_scorer.score_results(query, fused_results, config.RECENCY_SEMANTIC_WEIGHT, config.RECENCY_WEIGHT)`
   g. If rerank enabled (default from config): initialize CrossEncoderReranker (lazy), call `.rerank(query, fused_results, top_k or config.RERANK_TOP_K)`
   h. Convert to List[RetrievalResult]
   i. If include_citations: extract citations via CitationExtractor and attach to each result
   j. Return final results (top_k or config.DEFAULT_TOP_K)

   Log timing for each stage: "Hybrid search: vector={Xms}, bm25={Xms}, fusion={Xms}, rerank={Xms}, total={Xms}"

7. `ensure_bm25_index(self, tenant_id: str = "default") -> int` — Public method to build/rebuild BM25 index. Delegates to `self.bm25_index.build_index(tenant_id)`. Called manually after bulk ingestion or on schedule. Returns number of chunks indexed.

**Module-level convenience function:**
```python
def hybrid_search(
    query: str,
    tenant_id: str = "default",
    top_k: int = 10,
    **kwargs
) -> List[RetrievalResult]:
    """Convenience function for one-shot hybrid search."""
    retriever = HybridRetriever()
    return retriever.search(query, tenant_id, top_k=top_k, **kwargs)
```

**Important patterns:**
- Use `from execution.vector_db.bm25_index import BM25Index`
- Use `from execution.vector_db.reranking import CrossEncoderReranker`
- Use `from execution.vector_db.metadata_filters import build_filters`
- Use `from execution.vector_db.recency_scoring import RecencyScorer`
- Use `from execution.vector_db.citations import CitationExtractor`
- Use `from execution.vector_db.indexing import semantic_search` (existing)
- Use `from execution.vector_db.embeddings import EmbeddingClient`
- Use `from execution.vector_db.models import KnowledgeChunk, Document`
- Use `from execution.vector_db.connection import get_session, get_engine`
- Use `from execution.config import config` for defaults
- Use `from dataclasses import dataclass, field`
- Use `import time` and `import logging`
- Use `from typing import List, Dict, Optional, Tuple`
- Use `from datetime import datetime`

**Critical: Do NOT modify indexing.py** — the existing `semantic_search()` must remain unchanged. The filtered vector search in `_vector_search()` is a NEW code path that uses SQLAlchemy ORM directly (not raw SQL like indexing.py does). When no filters are provided, it delegates to the existing function.

**Graceful degradation:** If BM25 index is not available, fall back to vector-only search (no fusion). If CrossEncoder model fails to load, skip reranking and return RRF results directly. Log warnings in all degradation cases.
  </action>
  <verify>
Run: `python -c "
from execution.vector_db.retrieval import HybridRetriever, RetrievalResult, hybrid_search
from execution.config import config
print(f'RetrievalConfig loaded: RRF_K={config.retrieval.RRF_K}, RERANK_MODEL={config.retrieval.RERANK_MODEL_PROFILE}')
print(f'HybridRetriever importable: OK')
print(f'RetrievalResult fields: {[f.name for f in RetrievalResult.__dataclass_fields__.values()]}')
print('Import and config test passed')
"` — prints config values, fields list, success.

Run: `python -c "from execution.vector_db.indexing import semantic_search; print('Existing semantic_search still importable: OK')"` — confirms Phase 1 not broken.

Run: `python -c "from execution.config import config; print(f'retrieval config: {config.retrieval}')"` — RetrievalConfig accessible.
  </verify>
  <done>HybridRetriever orchestrator implementing full pipeline: vector search -> BM25 search -> RRF fusion -> recency boost -> CrossEncoder reranking -> citation extraction. RetrievalConfig added to execution/config.py with env_prefix="RETRIEVAL_". Graceful degradation when BM25 index or CrossEncoder unavailable. Existing semantic_search() preserved unchanged.</done>
</task>

<task type="auto">
  <name>Task 2: Package Exports + Integration Test</name>
  <files>execution/vector_db/__init__.py, scripts/test_retrieval.py</files>
  <action>
**Step 1: Update execution/vector_db/__init__.py**

Add imports for all Phase 2 modules. Add these import lines after the existing Phase 1 imports:

```python
# Phase 2: Retrieval Tools
from execution.vector_db.metadata_filters import MetadataFilter, build_filters
from execution.vector_db.recency_scoring import RecencyScorer
from execution.vector_db.citations import CitationExtractor, Citation
from execution.vector_db.bm25_index import BM25Index
from execution.vector_db.reranking import CrossEncoderReranker
from execution.vector_db.retrieval import HybridRetriever, RetrievalResult, hybrid_search
```

Add the new names to `__all__`:
```python
# Add to existing __all__ list:
"MetadataFilter", "build_filters",
"RecencyScorer",
"CitationExtractor", "Citation",
"BM25Index",
"CrossEncoderReranker",
"HybridRetriever", "RetrievalResult", "hybrid_search",
```

Update the module docstring to include Phase 2 exports in the usage example.

**CRITICAL:** Keep ALL existing Phase 1 imports intact. Only ADD new lines. The existing `from execution.vector_db.indexing import ...` etc. must remain.

**Step 2: Create scripts/test_retrieval.py**

Create an integration test script for Phase 2 retrieval. Follows the same pattern as `scripts/test_vectordb.py` (Phase 1).

```
Usage:
    python scripts/test_retrieval.py             # Run tests, keep data
    python scripts/test_retrieval.py --cleanup    # Run tests, then delete test data
```

**Test steps:**

1. **Prerequisites check** — Docker container running, OPENAI_API_KEY available (same as test_vectordb.py pattern).

2. **Database setup** — Call `init_db()`. Verify tables exist.

3. **Ingest test documents** — Ingest 4 documents with varying attributes:
   - Doc 1: Recent RSS article about "PostgreSQL pgvector HNSW indexing" (date_published = 2 days ago)
   - Doc 2: Old email newsletter about "remote work digital nomad cities" (date_published = 90 days ago)
   - Doc 3: Recent paper about "vector database benchmarks and performance" (date_published = 5 days ago)
   - Doc 4: Old RSS article about "Python web frameworks Django Flask" (date_published = 180 days ago)

   Use `ingest_document()` from Phase 1 for each. Use unique source_ids like "test-retr-001" through "test-retr-004".

4. **Build BM25 index** — Create BM25Index, call `build_index()`, verify returns correct count (should match total chunks from step 3). Print chunk count.

5. **Test hybrid search (basic)** — Call `hybrid_search("How does pgvector handle vector indexing?")`. Verify:
   - Returns non-empty results
   - Each result is a RetrievalResult with rrf_score > 0
   - Top result is related to pgvector/PostgreSQL (check title or content)
   - Print top 3 results with scores

6. **Test metadata filtering** — Call `HybridRetriever().search("database", source_types=["rss"])`. Verify:
   - All returned results have source_type "rss"
   - Results exclude email and paper documents
   - Print filtered result count and source types

7. **Test recency scoring** — Call `hybrid_search("latest database news", top_k=4)`. Verify:
   - Results are returned
   - Recent documents (docs 1, 3) should rank higher than old documents (docs 2, 4) due to recency boost on the trend keyword "latest"
   - Print results with recency_score and fused_score

8. **Test citation extraction** — Call `HybridRetriever().search("pgvector", include_citations=True, top_k=3)`. Verify:
   - Results have `citations` field populated (not None)
   - Each citation has citation_id in format "chunk_id.sentence_idx"
   - Print citation count and sample citation IDs

9. **Test reranking** — Search with reranking explicitly enabled, compare result ordering with reranking disabled:
   - `results_reranked = retriever.search("vector indexing performance", rerank=True)`
   - `results_no_rerank = retriever.search("vector indexing performance", rerank=False)`
   - Both should return results
   - Reranked results should have `rerank_score` field; non-reranked should not
   - Print both orderings

10. **Backward compatibility** — Import and call `semantic_search()` from Phase 1. Verify it still works:
    - `from execution.vector_db.indexing import semantic_search`
    - `results = semantic_search("pgvector", limit=3)`
    - Assert returns results
    - Print "Phase 1 backward compatibility: OK"

**Cleanup (optional --cleanup flag):**
- Delete test documents with source_ids matching "test-retr-*"
- Clear BM25 index
- Print cleanup summary

**Error handling:**
- Each test step wrapped in try/except
- Print PASS/FAIL per step
- Return exit code 0 if all pass, 1 on any failure
- If any step fails, continue remaining steps (don't short-circuit)

**sys.path setup:**
Same pattern as test_vectordb.py: `sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))`
  </action>
  <verify>
Run: `python -c "
from execution.vector_db import (
    HybridRetriever, RetrievalResult, hybrid_search,
    MetadataFilter, build_filters,
    RecencyScorer,
    CitationExtractor, Citation,
    BM25Index,
    CrossEncoderReranker,
    # Phase 1 exports still work
    semantic_search, create_hnsw_index, ingest_document,
)
print('All Phase 1 + Phase 2 exports importable: OK')
"` — all imports succeed.

Run: `python -c "from execution.vector_db.indexing import semantic_search; print('Phase 1 indexing module unchanged: OK')"` — backward compatibility.

If Docker + OPENAI_API_KEY available, run the full integration test:
`python scripts/test_retrieval.py`
Expected: All 10 steps pass.

If OpenAI budget unavailable, at minimum verify the script imports and runs prerequisite checks:
`python scripts/test_retrieval.py 2>&1 | head -20` — should show prerequisite checks, may fail at ingestion step due to API key.
  </verify>
  <done>
  1. __init__.py exports all Phase 2 modules (MetadataFilter, RecencyScorer, CitationExtractor, BM25Index, CrossEncoderReranker, HybridRetriever, RetrievalResult, hybrid_search)
  2. All Phase 1 exports preserved and working
  3. Integration test covers: hybrid search, metadata filtering, recency scoring, citations, reranking, and backward compatibility
  4. Test script follows same pattern as Phase 1 test (prerequisite checks, numbered steps, cleanup flag)
  </done>
</task>

</tasks>

<verification>
Full pipeline verification (requires Docker + OPENAI_API_KEY):
```bash
python scripts/test_retrieval.py
```

All steps should pass:
- Step 1: Prerequisites OK
- Step 2: Database setup OK
- Step 3: Test documents ingested (4 docs)
- Step 4: BM25 index built
- Step 5: Hybrid search returns relevant results
- Step 6: Metadata filtering scopes correctly
- Step 7: Recency boost prioritizes recent documents
- Step 8: Citations extracted with sentence-level IDs
- Step 9: Reranking changes result ordering
- Step 10: Phase 1 backward compatibility confirmed

Phase 1 integration test still passes:
```bash
python scripts/test_vectordb.py
```

Import check (no database needed):
```bash
python -c "from execution.vector_db import HybridRetriever, hybrid_search; print('OK')"
```
</verification>

<success_criteria>
1. HybridRetriever.search() executes full pipeline: vector + BM25 -> RRF -> recency -> rerank -> citations
2. hybrid_search() convenience function provides one-liner access
3. RetrievalConfig in config.py allows tuning all parameters via RETRIEVAL_* env vars
4. Metadata filters applied at SQL layer (WHERE clause), not post-retrieval
5. Graceful degradation: vector-only when BM25 unavailable, no-rerank when model fails
6. All Phase 2 modules exported from vector_db package
7. Integration test validates all 5 requirements (RETR-01 through RETR-05)
8. Phase 1 integration test (test_vectordb.py) still passes
9. Existing semantic_search() function in indexing.py unchanged
</success_criteria>

<output>
After completion, create `.planning/phases/02-retrieval-tools/02-03-SUMMARY.md`
</output>
